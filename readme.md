# üïµÔ∏è‚Äç‚ôÇÔ∏è Welcome to the GPT Exploration Extravaganza! üïµÔ∏è‚Äç‚ôÇÔ∏è

## Description

It's 2025, and yeah‚Ä¶ maybe it's about time I took a serious look at these GPT models everyone‚Äôs been raving about since forever. This project is my attempt to demystify the black box and understand how large language models (LLMs) really work‚Äîespecially that big ol‚Äô leap where they go from 'pretrained' to 'actually useful for a task.'

In true stubborn fashion, I decided to build and align one myself before checking how the pros do it. Spoiler alert: Hugging Face crushed it. If this were a song, it‚Äôd be "I fought the law, and the law won."

The goal? See if I can get a model to learn how to answer questions about the world of Sherlock Holmes‚Äîsolving mysteries, referencing obscure plot points, and navigating the foggy streets of Victorian London with proper deductive flair. Let‚Äôs just say: elementary, it is not.

## Install

Installing this project is easier than understanding quantum physics (or GPT, for that matter). Just follow these simple steps:

1. Clone the repo: 

    ```bash
    git clone https://github.com/TheRealMarVin/gp2.git
    ```
    
2. Navigate to the project directory: 

    ```bash
    cd gpt2
    ```
    
3. Install the necessary dependencies (pro tip: use a virtual environment so you don't accidentally break the space-time continuum):

    ```bash
    pip install -r requirements.txt
    ```
   You will also need to install the latest PyTorch version

## Running

Write something