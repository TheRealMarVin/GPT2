# 🕵️‍♂️ Welcome to the GPT Exploration Extravaganza! 🕵️‍♂️

## Description

It's 2025, and yeah… maybe it's about time I took a serious look at these GPT models everyone’s been raving about since forever. This project is my attempt to demystify the black box and understand how large language models (LLMs) really work—especially that big ol’ leap where they go from 'pretrained' to 'actually useful for a task.'

In true stubborn fashion, I decided to build and align one myself before checking how the pros do it. Spoiler alert: Hugging Face crushed it. If this were a song, it’d be "I fought the law, and the law won."

The goal? See if I can get a model to learn how to answer questions about the world of Sherlock Holmes—solving mysteries, referencing obscure plot points, and navigating the foggy streets of Victorian London with proper deductive flair. Let’s just say: elementary, it is not.

## Install

Installing this project is easier than understanding quantum physics (or GPT internals, for that matter). Just follow these simple steps:

1. Clone the repo from the depths of GitHub:

   ```bash
   git clone https://github.com/TheRealMarVin/gpt2.git
   ```

2. Enter the lair:

   ```bash
   cd gpt2
   ```

3. Install the required Python packages (pro tip: use a virtual environment to keep your machine from exploding):

   ```bash
   pip install -r requirements.txt
   ```

4. Install the latest version of PyTorch separately (with or without CUDA, depending on your machine). Check it out at: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)

> 💡 Python 3.11+ is recommended. Older versions might make Sherlock frown in disapproval.

## Running

Write something

