# ðŸ•µï¸â€â™‚ï¸ Welcome to the GPT Exploration Extravaganza! ðŸ•µï¸â€â™‚ï¸

## Description

It's 2025, and yeahâ€¦ maybe it's about time I took a serious look at these GPT models everyoneâ€™s been raving about since forever. This project is my attempt to demystify the black box and understand how large language models (LLMs) really workâ€”especially that big olâ€™ leap where they go from 'pretrained' to 'actually useful for a task.'

In true stubborn fashion, I decided to build and align one myself before checking how the pros do it. Spoiler alert: Hugging Face crushed it. If this were a song, itâ€™d be "I fought the law, and the law won."

The goal? See if I can get a model to learn how to answer questions about the world of Sherlock Holmesâ€”solving mysteries, referencing obscure plot points, and navigating the foggy streets of Victorian London with proper deductive flair. Letâ€™s just say: elementary, it is not.

## Install

Installing this project is easier than understanding quantum physics (or GPT internals, for that matter). Just follow these simple steps:

1. Clone the repo from the depths of GitHub:

   ```bash
   git clone https://github.com/TheRealMarVin/gpt2.git
   ```

2. Enter the lair:

   ```bash
   cd gpt2
   ```

3. Install the required Python packages (pro tip: use a virtual environment to keep your machine from exploding):

   ```bash
   pip install -r requirements.txt
   ```

4. Install the latest version of PyTorch separately (with or without CUDA, depending on your machine). Check it out at: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)

> ðŸ’¡ Python 3.11+ is recommended. Older versions might make Sherlock frown in disapproval.

## Running

Write something

